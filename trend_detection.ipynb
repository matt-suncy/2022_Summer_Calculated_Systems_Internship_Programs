{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c28a8f28-26d7-46a7-a999-232874ba9fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor: Matthew Sun\\nDate: August 24, 2022\\nDescription: Program pulls messages from stream and displays word count, \\npotential trends, and hashtag relations across a moving time window.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Author: Matthew Sun\n",
    "Date: August 24, 2022\n",
    "Description: Program pulls messages from stream and displays word count, \n",
    "potential trends, and hashtag relations across a moving time window.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c683a496-39dc-469b-88ff-2e38b4d108e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import TimeoutError\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "from math import*\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a54896e-a0fe-442d-8b1b-2179671310f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_counter_to_dict(counter):\n",
    "    '''\n",
    "    Converts a counter.most_common(n) list of tuples into a dictionary\n",
    "    Parameters: counter - a counter.most_common(n) list of tuples\n",
    "    Returns: counterDict - a dictionary with words and their frequency\n",
    "    '''\n",
    "    counterDict = {}\n",
    "    for pair in list(counter):\n",
    "        counterDict[pair[0]] = pair[1]\n",
    "    \n",
    "    return counterDict\n",
    "            \n",
    "def rank_hashtags(counter):\n",
    "    '''\n",
    "    Converts a counter into a dictionary with addtional information including rank \n",
    "    and relative mentions\n",
    "    Parameters: counter - a counter\n",
    "    Returns: rankDict - a dictionary with additional information\n",
    "    '''\n",
    "    rankDict = {}\n",
    "    rankAssigned = 1\n",
    "    hashtagTotal = sum(counter.values())\n",
    "    \n",
    "    for word in counter:\n",
    "        rankDict[word] = {'rank': rankAssigned, 'total_mentions': counter[word], 'relative_mentions': round(float(counter[word] / hashtagTotal), 4)}\n",
    "        rankAssigned += 1\n",
    "    \n",
    "    return rankDict\n",
    "\n",
    "def lin_slope_per_word(counterQueue, n):\n",
    "    '''\n",
    "    Converts a counter.most_common(n) list of tuples into a dictionary\n",
    "    Parameters: counterQueue - a deque of counters\n",
    "    n - an integer determining how many hashtags are in the returned dictionary\n",
    "    Returns: counterDict - a dictionary with words and their frequency\n",
    "    '''\n",
    "    \n",
    "    queueSum = Counter()\n",
    "    for counter in counterQueue:\n",
    "        queueSum += counter\n",
    "    \n",
    "    slopeDict = {}\n",
    "\n",
    "    for pair in queueSum.most_common(n):\n",
    "        # Process for calculating the slope value for linear regression\n",
    "        xVals = []\n",
    "        yVals = []\n",
    "        for i in range(len(counterQueue)):\n",
    "            xVals.append(i + 1)\n",
    "            if pair[0] in counterQueue[i]:\n",
    "                yVals.append(counterQueue[i][pair[0]])\n",
    "            else:\n",
    "                yVals.append(0)\n",
    "        xMean = sum(xVals) / len(xVals)\n",
    "        yMean = sum(yVals) / len(yVals)\n",
    "\n",
    "        xySum = 0\n",
    "        xxSum = 0\n",
    "        for i in range(len(xVals)):\n",
    "            xySum += (xVals[i] - xMean) * (yVals[i] - yMean)\n",
    "            xxSum += (xVals[i] - xMean) ** 2\n",
    "\n",
    "        slopeDict[pair[0]] = round(xySum / xxSum, 4)\n",
    "    \n",
    "    return slopeDict\n",
    "\n",
    "def avg_lin_slope(counterQueue, n):\n",
    "    '''\n",
    "    Calculates the average linear slope value for the most common n words from the\n",
    "    sum of the counterQueue\n",
    "    Parameters: counterQueue - a deque of counters\n",
    "    n - an integer determining how many hashtags are in the returned dictionary\n",
    "    Returns: round(slopeSum / n, 4) - the average slope value of the n most popular\n",
    "    words rounded to 4 decimals\n",
    "    '''\n",
    "    \n",
    "    queueSum = Counter()\n",
    "    for counter in counterQueue:\n",
    "        queueSum += counter\n",
    "    \n",
    "    slopeSum = 0\n",
    "    for pair in queueSum.most_common(n):\n",
    "        xVals = []\n",
    "        yVals = []\n",
    "        for i in range(len(counterQueue)):\n",
    "            xVals.append(i + 1)\n",
    "            if pair[0] in counterQueue[i]:\n",
    "                yVals.append(counterQueue[i][pair[0]])\n",
    "            else:\n",
    "                yVals.append(0)\n",
    "        xMean = sum(xVals) / len(xVals)\n",
    "        yMean = sum(yVals) / len(yVals)\n",
    "\n",
    "        xySum = 0\n",
    "        xxSum = 0\n",
    "        for i in range(len(xVals)):\n",
    "            xySum += (xVals[i] - xMean) * (yVals[i] - yMean)\n",
    "            xxSum += (xVals[i] - xMean) ** 2\n",
    "\n",
    "        slopeSum += xySum / xxSum       \n",
    "    \n",
    "    return round(slopeSum / n, 4)\n",
    "\n",
    "def square_rooted(x):\n",
    "    '''\n",
    "    Returns a list with its values squared\n",
    "    Parameters: x - a list of numbers\n",
    "    Returns: sqrt(sum([a*a for a in x])) - list with squared values\n",
    "    '''\n",
    "             \n",
    "    return sqrt(sum([a*a for a in x]))\n",
    " \n",
    "def cosine_similarity(x,y):\n",
    "    '''\n",
    "    Calculates the cosine similarity between two vectors\n",
    "    Parameters: x - a list of numbers\n",
    "    y - a list of numbers\n",
    "    Returns: round(numerator/float(denominator), 4) - cosine similarity rounded to\n",
    "    4 decimals\n",
    "    '''\n",
    "    \n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return round(numerator/float(denominator), 4)\n",
    "\n",
    "def related_hashtags_dict(counterQueue, targetWords):\n",
    "    '''\n",
    "    Creates a dictionary were each key is a potential trend word and the value is a\n",
    "    list of the other most popular hashtags and their cosine similarity score to\n",
    "    that key in descending order\n",
    "    Parameters: counterQueue - a queue of counters\n",
    "    targetWords - a list of tuples in the format [('hashtag word x', M), ...] where M is\n",
    "    the slope value of 'hashtag word x'\n",
    "    Returns: similairtyDict - dictionary of potential trend words and their cosine\n",
    "    similarities to the other most popular hashtags\n",
    "    '''\n",
    "    \n",
    "    queueSum = Counter()\n",
    "    # A sum of all counters is neccessary to determine what words are most popular\n",
    "    # overall.\n",
    "    for counter in counterQueue:\n",
    "        queueSum += counter\n",
    "    \n",
    "    timeWindows = []\n",
    "    # Seperates the queue into 5 seperate sections\n",
    "    for i in range(0, len(counterQueue), 5):\n",
    "        window = []\n",
    "        for j in range(i, i+5):\n",
    "            window.append(counterQueue[j])\n",
    "        timeWindows.append(window)\n",
    "    \n",
    "    slopeDictList = []\n",
    "    # slopeDictList will contain 5 dictionaries where each dictionary contains a\n",
    "    # word as a key and their slope value as the value.\n",
    "    for q in timeWindows:\n",
    "        slopeDictList.append(lin_slope_per_word(q, 100))\n",
    "\n",
    "    mostCommonWords = queueSum.most_common(50)\n",
    "    commonWordSlopes = {}\n",
    "    # Making a dictionary with the top 50 hashtags using the slopeDictList.\n",
    "    # commonWordSlopes will have hashtag words as keys and a list of 5 slope values\n",
    "    # from each time window as the values.\n",
    "    # If a word is not found in one of the elements of timeWindows, it will be\n",
    "    # assigned a 0 in that posistion in tempSlopeList.\n",
    "    for pair in mostCommonWords:\n",
    "\n",
    "        tempSlopeList = []\n",
    "        for d in slopeDictList:\n",
    "            if pair[0] in d:\n",
    "                tempSlopeList.append(d[pair[0]])\n",
    "            else:\n",
    "                tempSlopeList.append(0)\n",
    "\n",
    "        commonWordSlopes[pair[0]] = tempSlopeList\n",
    "\n",
    "    # Similarity test.\n",
    "    # Each trending word is compared with all other words. \n",
    "    similarityDict = {}\n",
    "    for targetWord in targetWords:\n",
    "        if targetWord[0] in commonWordSlopes:\n",
    "            cosSimList = []\n",
    "            for pair in mostCommonWords:\n",
    "                if pair[0] != targetWord[0]:\n",
    "                    cosSim = cosine_similarity(commonWordSlopes[targetWord[0]], commonWordSlopes[pair[0]])\n",
    "                    cosSimList.append((pair[0], cosSim))\n",
    "            similarityDict[targetWord[0]] = cosSimList \n",
    "\n",
    "    for word in similarityDict:\n",
    "        similarityDict[word].sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return similarityDict\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079325cd-a16d-46a4-83df-9799c7e80030",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSGLIST = []\n",
    "# project_id = 'stingray-295922'\n",
    "# subscription_id = 'stringray-intelligence-dev' \n",
    "def callback(message: pubsub_v1.subscriber.message.Message) -> None:\n",
    "    '''\n",
    "    Function for requesting and acknowledging messages from stream\n",
    "    '''\n",
    "    \n",
    "    global MSGLIST\n",
    "\n",
    "    data = message.data.decode()\n",
    "    dictionary = json.loads(data)\n",
    "    MSGLIST.append(dictionary)\n",
    "\n",
    "    message.ack()\n",
    "    \n",
    "def word_count_hashtags(text):\n",
    "    '''\n",
    "    Counts all hashtags in a given string\n",
    "    Parameters: text - a string\n",
    "    Returns: hashtagCount - a counter with hashtags and their number of mentions\n",
    "    '''\n",
    "    \n",
    "    hashtagCount = Counter()\n",
    "    for x in text:\n",
    "        for hashTag in re.findall('#(\\w+)', x):\n",
    "            hashtagCount[hashTag.casefold()] += 1\n",
    "    \n",
    "    return hashtagCount\n",
    "\n",
    "def preprocess(str):\n",
    "    '''\n",
    "    Puts a string through the proprocess functions which will remove any symbols\n",
    "    used in replies, remove urls, and remove forwards\n",
    "    '''\n",
    "    str = remove_forward(str)\n",
    "    str = remove_url(str)\n",
    "    str = remove_reply(str)\n",
    "    \n",
    "    return str\n",
    "\n",
    "def remove_emojis(str):\n",
    "    emojis = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    try:\n",
    "        str = emojis.sub(r'', str)\n",
    "        return str\n",
    "    except:\n",
    "        return str\n",
    "\n",
    "def remove_forward(str):\n",
    "    try:\n",
    "        str = re.sub(r'//@.+?:', '', str)\n",
    "        return str\n",
    "    except:\n",
    "        return str\n",
    "    \n",
    "def remove_url(str):\n",
    "    try:\n",
    "        str = re.sub(r\"http\\S+\", '', str)\n",
    "        return str\n",
    "    except:\n",
    "        return str\n",
    "\n",
    "def remove_reply(str):\n",
    "    try:\n",
    "        str = re.sub(r'回复@.+?:', '', str)\n",
    "        return str\n",
    "    except:\n",
    "        return str\n",
    "\n",
    "def translate_zhHans_to_eng(chineseWord):\n",
    "    '''\n",
    "    Translate string from simplified Chinese to English\n",
    "    Parameters: chineseWord - a string of chinese characters\n",
    "    Returrns: engWord - translated string in English\n",
    "    '''\n",
    "    \n",
    "    url = 'http://35.243.155.154:8080/api/translate'\n",
    "    myObj = {\"q\": chineseWord,\"source\":\"zh-Hans\",\"target\":\"en\"}\n",
    "    r = requests.post(url, json=myObj)\n",
    "    try:\n",
    "        engWord = json.loads(r.text)['translatedText']\n",
    "        return engWord\n",
    "    except KeyError:\n",
    "        return chineseWord\n",
    "    \n",
    "\n",
    "def pull_count():\n",
    "    '''\n",
    "    Pulls 100 messages per iteration from stream for certain amount of time and word\n",
    "    counts all hashtags\n",
    "    Parameters: None\n",
    "    Returns: wordCounter - a counter containing hashtags words and their frequencies\n",
    "    '''\n",
    "    \n",
    "    global MSGLIST\n",
    "    wordCounter = Counter()\n",
    "    # Timer set to pull for 60 seconds\n",
    "    t = int(time.time()) + 60\n",
    "    while (time.time() <= t): \n",
    "        \n",
    "        textList = []\n",
    "        subscriber = pubsub_v1.SubscriberClient()\n",
    "        subscription_path = 'projects/stingray-295922/subscriptions/stingray-intelligence-dev'\n",
    "        timeout = 1.5\n",
    "        flow_control = pubsub_v1.types.FlowControl(max_messages=100)\n",
    "        streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback, flow_control=flow_control)\n",
    "\n",
    "        with subscriber:\n",
    "            try:\n",
    "                streaming_pull_future.result(timeout=timeout)\n",
    "            except TimeoutError:\n",
    "                streaming_pull_future.cancel()  # Trigger the shutdown.\n",
    "                streaming_pull_future.result()  # Block until the shutdown is complete.\n",
    "\n",
    "        for msg in MSGLIST:\n",
    "            if 'status' in msg['text'].keys():\n",
    "                text = msg['text']['status']['text']\n",
    "                text = preprocess(text)\n",
    "                textList.append(text)\n",
    "            elif 'comment' in msg['text'].keys():\n",
    "                text = msg['text']['comment']['status']['text']\n",
    "                text = preprocess(text)\n",
    "                textList.append(text)\n",
    "\n",
    "        wordCounter += word_count_hashtags(textList)\n",
    "        \n",
    "        MSGLIST = []\n",
    "        textList = []\n",
    "    \n",
    "    return wordCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413ea3d-79ea-4a79-9d8a-0accd5cfc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    Put counters from every minute into queue and every minute the oldest counter will\n",
    "    be taken out. Display the sum of all counters of the last 5 minutes. bigQueue will \n",
    "    contain the information from the last 25 minutes which is needed to compare with \n",
    "    the most recent 5 minutes of information in order to find potential trends.\n",
    "    Parameters: None\n",
    "    Returns: None\n",
    "    '''\n",
    "\n",
    "    bigQueue = deque()\n",
    "    maxLength = 25\n",
    "\n",
    "    chnQueue = deque()\n",
    "    chnQueueSum = Counter()\n",
    "    \n",
    "    for i in range(5):\n",
    "        chnCounter = pull_count()\n",
    "        chnQueue.append(chnCounter)\n",
    "        chnQueueSum += chnCounter\n",
    "        \n",
    "        if len(bigQueue) < maxLength:\n",
    "            bigQueue.append(chnCounter)\n",
    "        \n",
    "    print('Initial: ' + str(chnQueueSum.most_common(100)) + '\\n')\n",
    "\n",
    "\n",
    "    timesUpdated = 0\n",
    "    potentialRisers = []\n",
    "    while True:\n",
    "        # Updating Chinese queue\n",
    "        chnQueueSum -= chnQueue[0]\n",
    "        chnCounter = pull_count()\n",
    "        chnQueueSum += chnCounter\n",
    "        chnQueue.popleft()\n",
    "        chnQueue.append(chnCounter)\n",
    "\n",
    "        timesUpdated += 1\n",
    "        print('Update ' + str(timesUpdated) + ': ' + str(chnQueueSum.most_common(100)) + '\\n')\n",
    "        \n",
    "        if len(bigQueue) < maxLength:\n",
    "            bigQueue.append(chnCounter)\n",
    "        else:\n",
    "            bigQueue.popleft()\n",
    "            bigQueue.append(chnCounter)\n",
    "        \n",
    "        # Once bigQueue reaches wanted length the program will look for potential trends\n",
    "        if len(bigQueue) == maxLength:\n",
    "            slopeDict = lin_slope_per_word(chnQueue, 50)\n",
    "            avgSlope = avg_lin_slope(bigQueue, 100)\n",
    "            \n",
    "            slopeSum = 0\n",
    "            cutOff = 0\n",
    "            # First cut: only words that have a greater slope value than avgSlope move on.\n",
    "            for word in slopeDict:\n",
    "                if slopeDict[word] > avgSlope:\n",
    "                    slopeSum += slopeDict[word]\n",
    "                    potentialRisers.append((word, slopeDict[word]))\n",
    "            \n",
    "            cutOff = slopeSum / len(potentialRisers)\n",
    "            trendingWords = []\n",
    "            # To further reduce noise, only words with above average slope values among\n",
    "            # potentiRisers will be added to trendingWords,\n",
    "            for pair in potentialRisers:\n",
    "                if pair[1] > cutOff and pair[1] > 0:\n",
    "                    trendingWords.append(pair)\n",
    "            \n",
    "            print(avgSlope)\n",
    "            print(cutOff)\n",
    "            trendingWords.sort(key=lambda pair: pair[1], reverse=True)\n",
    "            print('Potential trends:', trendingWords, '\\n')\n",
    "            \n",
    "            # Finding related hashtags\n",
    "            if len(trendingWords) > 0:\n",
    "                try:\n",
    "                    hashtagRelations = related_hashtags_dict(bigQueue, trendingWords)\n",
    "                    relationsDict = {}\n",
    "                    # The 3 most related hashtags for each potential trend are displayed.\n",
    "                    for word in hashtagRelations:\n",
    "                        relationsDict[word] = hashtagRelations[word][:3]\n",
    "                    print('Hashtag relations:', relationsDict, '\\n\\n')\n",
    "                # Occasionally the program will run into a zero division error.\n",
    "                # This is most likely because words do not appear in certain time windows.\n",
    "                except ZeroDivisionError:\n",
    "                    print('Unable to generate hashtag relations: zero division error')\n",
    "                    \n",
    "            potentialRisers = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2a9001-687f-4d9a-acd5-0a83a33f7071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
